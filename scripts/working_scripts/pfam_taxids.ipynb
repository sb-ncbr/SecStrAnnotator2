{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import no_gap_align\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(no_gap_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = Path('/home/adam/Workspace/C#/SecStrAnnot2_data/SecStrAPI/CytochromesP450-20200707-full/Pfam_HMM')\n",
    "UNIPROTID_FILE = DIR/'PF00067.rp35.ids-wo_version'\n",
    "CHUNK_SIZE = 100  # number of accessions requested from UniProt API in one call\n",
    "API_URL = 'http://www.ebi.ac.uk/proteins/api/proteins?accession={uniprots}'\n",
    "PARTIAL_DIR = DIR/'partial_taxonomy'\n",
    "MISSING_FILE = DIR/'missing_uniprots.txt'\n",
    "EXTRA_FILE = DIR/'extra_uniprots.txt'\n",
    "MISSING_ORGANISM_FILE = DIR/'missing_organism.txt'\n",
    "TOGETHER_FILE = DIR/'taxonomy.json'\n",
    "ALIGNED_SEQUENCES_JSON = DIR/'sequences.json'\n",
    "LOGO_RANGE = slice(87, 111)\n",
    "AMINO_ACIDS = list('ACDEFGHIKLMNPQRSTVWXY-')\n",
    "AMINO_INDEX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(lst: List[object], chunk_size: int) -> List[List[object]]:\n",
    "    n_chunks = math.ceil(len(lst) / chunk_size)\n",
    "    chunks = [lst[i*chunk_size:(i+1)*chunk_size] for i in range(n_chunks)]\n",
    "    assert sum(len(c) for c in chunks)==len(lst)\n",
    "    return chunks\n",
    "\n",
    "def add_to_file(lst: List[str], filename: Path) -> None:\n",
    "    if len(lst) > 0:\n",
    "        with open(filename, 'a') as w:\n",
    "            for elem in lst:\n",
    "                w.write(elem)\n",
    "                w.write('\\n')\n",
    "\n",
    "def select_uniprots_from_taxon(uniprot_to_organism: Dict[str,dict], taxon: str, uniprots: Optional[List[str]] = None) -> List[str]:\n",
    "    uniprots = uniprots or uniprot_to_organism.keys()\n",
    "    return [uni for uni in uniprots if taxon in uniprot_to_organism.get(uni, {}).get('lineage', [])]\n",
    "\n",
    "def remove_inserts(sequence: str) -> str:\n",
    "    sequence = sequence.replace('.', '')\n",
    "    return ''.join(c for c in sequence if c.isupper() or c=='-')\n",
    "\n",
    "def save_fasta(sequences: Dict[str, str], filename: str, cut_range: slice = slice(None, None)) -> None:\n",
    "    with open(filename, 'w') as w:\n",
    "        for name, seq in sequences.items():\n",
    "            w.write(f'>{name}\\n{seq[cut_range]}\\n\\n')\n",
    "\n",
    "def read_sequences_from_stockholm(filename: str, dot_as_dash: bool = False) -> Dict[str, str]:\n",
    "    seq_lens = set()\n",
    "    sequences = {}\n",
    "    with open(filename) as r:\n",
    "        for line in r:\n",
    "            line = line.strip()\n",
    "            if not line.startswith('#') and not line.startswith('//'):\n",
    "                name, seq = line.split()\n",
    "                uni_v, range = name.split('/')\n",
    "                uni = uni_v.split('.')[0]\n",
    "                if dot_as_dash:\n",
    "                    seq = seq.replace('.', '-')\n",
    "                seq = remove_inserts(seq)\n",
    "                sequences[name] = seq\n",
    "                seq_lens.add(len(seq))\n",
    "    print(seq_lens)\n",
    "    assert len(seq_lens) <= 1\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{463}\n",
      "{557}\n"
     ]
    }
   ],
   "source": [
    "# LOAD UNIPROTIDS AND SEQUENCES\n",
    "sequences = read_sequences_from_stockholm(DIR/'PF00067.rp35')\n",
    "with open(ALIGNED_SEQUENCES_JSON, 'w') as w:\n",
    "    json.dump(sequences, w)\n",
    "sequences_seed = read_sequences_from_stockholm(DIR/'PF00067.seed', dot_as_dash=True)\n",
    "\n",
    "uniprots_inlc_duplicates = [name.split('/')[0].split('.')[0] for name in sequences.keys()]\n",
    "uniprots = sorted(set(uniprots_inlc_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DOWNLOAD ORGANISM DATA FOR UNIPROTIDS\n",
    "\n",
    "# with open(UNIPROTID_FILE) as r:\n",
    "#     uniprots = sorted(set(line.strip() for line in r))\n",
    "\n",
    "uniprots_chunks = make_chunks(uniprots, CHUNK_SIZE)\n",
    "\n",
    "Path(PARTIAL_DIR).mkdir(exist_ok=True, parents=True)\n",
    "for i, chunk in enumerate(uniprots_chunks):\n",
    "    # print('chunk', i)\n",
    "    url = API_URL.format(uniprots=','.join(chunk))\n",
    "    response = requests.get(url, headers={'Accept':'application/json'}).text\n",
    "    js = json.loads(response)\n",
    "\n",
    "    retrieved = [protein['accession'] for protein in js]\n",
    "    missing = sorted(set(chunk) - set(retrieved))\n",
    "    extra = sorted(set(retrieved) - set(chunk))\n",
    "    missing_organism = []\n",
    "    chunk_result = {}\n",
    "    for protein in js:\n",
    "        uni = protein['accession']\n",
    "        organism = protein.get('organism', None)\n",
    "        if organism is not None:\n",
    "            chunk_result[uni] = organism\n",
    "        else:\n",
    "            missing_organism.append(uni)\n",
    "    add_to_file(missing, MISSING_FILE)\n",
    "    add_to_file(extra, EXTRA_FILE)\n",
    "    add_to_file(missing_organism, MISSING_ORGANISM_FILE)\n",
    "    with open(Path(PARTIAL_DIR, f'chunk_{i:03}.json'), 'w') as w:\n",
    "        json.dump(chunk_result, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE DOWNLOADED DATA INTO ONE FILE\n",
    "if PARTIAL_DIR.is_dir():\n",
    "    together = {}\n",
    "    for file in sorted(PARTIAL_DIR.glob('chunk_*.json')):\n",
    "        js = json.loads(file.read_text())\n",
    "        together.update(js)\n",
    "    with open(TOGETHER_FILE, 'w') as w:\n",
    "        json.dump(together, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PREVIOUSLY DOWNLOADED ORGANISM DATA\n",
    "with open(TOGETHER_FILE) as r:\n",
    "    together = json.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eukaryota  54453\nBacteria    8087\nArchaea       98\nViruses       31\n----------------\n           63397\n"
     ]
    }
   ],
   "source": [
    "# PRINT SUPERKINGDOM DISTRIBUTION - Taking each UniProtID only once\n",
    "for taxon in ['Eukaryota', 'Bacteria', 'Archaea', 'Viruses']:\n",
    "    taxon_unis = select_uniprots_from_taxon(together, taxon)\n",
    "    print(f'{taxon:10} {len(taxon_unis):5}')\n",
    "print('-'*16)\n",
    "print(f'{len(uniprots):16}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eukaryota  59809\nBacteria    8834\nArchaea      102\nViruses       31\nFailed       814\n----------------\n           69590\n"
     ]
    }
   ],
   "source": [
    "# PRINT SUPERKINGDOM DISTRIBUTION - Taking each UniProtID repeatedly, if found more times in alignment\n",
    "total = 0\n",
    "for taxon in ['Eukaryota', 'Bacteria', 'Archaea', 'Viruses']:\n",
    "    taxon_unis = select_uniprots_from_taxon(together, taxon, uniprots_inlc_duplicates)\n",
    "    total += len(taxon_unis)\n",
    "    print(f'{taxon:10} {len(taxon_unis):5}')\n",
    "print(f'{\"Failed\":10} {len(uniprots_inlc_duplicates)-total:5}')\n",
    "print('-'*16)\n",
    "print(f'{len(uniprots_inlc_duplicates):16}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVE ALIGNMENTS IN FASTA (INCL. SELECTED EUKA, BACT...)\n",
    "# with open(ALIGNED_SEQUENCES_FILE) as r:\n",
    "#     sequences = json.load(r)\n",
    "euka = set(select_uniprots_from_taxon(together, 'Eukaryota'))\n",
    "bact = set(select_uniprots_from_taxon(together, 'Bacteria'))\n",
    "sequences_euka = {name: seq for name, seq in sequences.items() if name.split('/')[0].split('.')[0] in euka}\n",
    "sequences_bact = {name: seq for name, seq in sequences.items() if name.split('/')[0].split('.')[0] in bact}\n",
    "save_fasta(sequences, DIR/'aln.fasta', cut_range=LOGO_RANGE)\n",
    "save_fasta(sequences_euka, DIR/'aln-euka.fasta', cut_range=LOGO_RANGE)\n",
    "save_fasta(sequences_bact, DIR/'aln-bact.fasta', cut_range=LOGO_RANGE)\n",
    "save_fasta(sequences_seed, DIR/'aln-seed.fasta', cut_range=LOGO_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Warning: Character 'X' is not in color_dict. Using black.\n",
      " Warning: Character 'X' is not in color_dict. Using black.\n"
     ]
    }
   ],
   "source": [
    "# CREATE LOGOS\n",
    "no_gap_align.run_logomaker(DIR/'aln.fasta', DIR/'logo.png', first_index=39, title='Helix C (Pfam RP35)')\n",
    "no_gap_align.run_logomaker(DIR/'aln-euka.fasta', DIR/'logo-euka.png', first_index=39, title='Helix C (Pfam RP35 Eukaryota)')\n",
    "no_gap_align.run_logomaker(DIR/'aln-bact.fasta', DIR/'logo-bact.png', first_index=39, title='Helix C (Pfam RP35 Bacteria)')\n",
    "no_gap_align.run_logomaker(DIR/'aln-seed.fasta', DIR/'logo-seed.png', first_index=39, title='Helix C (Pfam seed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fasta(sequences_seed, DIR/'aln-seed.fasta', cut_range=slice(LOGO_RANGE.start+10, LOGO_RANGE.stop+10))\n",
    "no_gap_align.run_logomaker(DIR/'aln-seed.fasta', DIR/'logo-seed.png', first_index=39, title='Helix C (Pfam seed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CP27A_HUMAN/61-526\nC11B2_MOUSE/42-496\nC11B1_BOVIN/42-499\nCP11A_ONCMY/50-507\nCP11A_HUMAN/52-511\nCP11A_BOVIN/52-510\nCP19A_MOUSE/48-488\nCP19A_CHICK/47-487\nCP4F3_HUMAN/52-515\nCP4F1_RAT/52-515\nCP4B1_HUMAN/47-501\nCP4AA_RAT/52-504\nCP4A2_RAT/52-499\nCP4C1_BLADI/37-502\nCP4D1_DROME/34-507\nCP3A6_RABIT/37-491\nCP3A1_RAT/39-494\nCP6B1_PAPPO/31-495\nCP6A2_DROME/32-502\nCP6A1_MUSDO/35-500\nPID6_FUSSO/51-503\nG3XMQ0_ASPNA/36-509\nCP7A1_HUMAN/32-497\nCP51_YEAST/57-521\nCPXI_BACMB/12-406\nCP17A_MOUSE/28-492\nCP17A_HUMAN/28-493\nCP17A_BOVIN/28-493\nCP17A_ONCMY/34-500\nCP17A_CHICK/33-496\nCP2D1_RAT/37-497\nCP2D4_RAT/37-497\nCP2DE_BOVIN/37-497\nCP2F1_HUMAN/31-488\nCP2B1_RAT/31-488\nCP2G1_RABIT/34-491\nCP2A1_RAT/33-489\nCP2H1_CHICK/33-488\nCP2E1_HUMAN/33-489\nCP2CN_RAT/34-491\nCP270_RAT/30-486\nCP2C3_RABIT/30-486\nCP2C1_RABIT/30-487\nCP2C7_RAT/30-487\nCP2CC_RAT/30-487\nC75A2_SOLME/37-498\nC76A2_SOLME/36-500\nTCMO_HELTU/34-499\nC77A1_SOLME/28-497\nC77A2_SOLME/43-509\n"
     ]
    }
   ],
   "source": [
    "print(*sequences_seed.keys(), sep='\\n')"
   ]
  }
 ]
}